<!DOCTYPE html>
<html>
  <head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#33363b">
    <meta name="msapplication-TileColor" content="#33363b">
    
    
    
    
    <meta name="keywords" content="Life, Python, C++, ML/DL/CV">
    
    
    <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png">
    
    
    <link rel="icon" type="image/png" sizes="192x192" href="/favicons/android-chrome-192x192.png">
    
    
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    
    
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    
    
    <link rel="mask-icon" href="/favicons/safari-pinned-tab.svg" color="#33363b">
    
    
    <link rel="manifest" href="/favicons/site.webmanifest">
    
    
    <meta name="msapplication-config" content="/favicons/browserconfig.xml">
    
    
    <link rel="alternate" href="/atom.xml" title="Huris' HeapStack" type="application/atom+xml">
    
    
    <link rel="shortcut icon" type="image/x-icon" href="/favicons/favicon.ico">
    
    
    <link rel="stylesheet" type="text/css" href="/css/normalize.css">
    <link rel="stylesheet" type="text/css" href="/css/index.css">
    
    <link rel="stylesheet" type="text/css" href="/css/sidebar.css">
    
    
<link rel="stylesheet" type="text/css" href="/css/page.css">
<link rel="stylesheet" type="text/css" href="/css/post.css">

    <link rel="stylesheet" type="text/css" href="/css/custom.css">
    <link rel="stylesheet" type="text/css" href="/css/github.css">
    <link rel="stylesheet" type="text/css" href="/css/lightgallery.min.css">
    <script type="text/javascript" src="/js/jquery.min.js"></script>
    <script defer type="text/javascript" src="/js/util.js"></script>
    <script defer type="text/javascript" src="/js/clipboard.min.js"></script>
    <script defer type="text/javascript" src="/js/scrollspy.js"></script>
    <script defer type="text/javascript" src="/js/fontawesome-all.min.js"></script>
    <script defer type="text/javascript" src="/js/lightgallery.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-fullscreen.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-hash.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-pager.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-thumbnail.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-zoom.min.js"></script>
    
    <script defer src="/js/busuanzi.pure.mini.js"></script>
    
    
    <script defer type="text/javascript" src="/js/search.js"></script>
    <script type="text/javascript">
    $(document).ready(function () {
      var searchPath = "search.xml";
      if (searchPath.length === 0) {
        searchPath = "search.xml";
      }
      var path = "/" + searchPath;
      searchFunc(path, "search-input", "search-result");
    });
    </script>
    
    
    
    <script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      }
    });
    </script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += " has-jax";
      }
    });
    </script>
    
    
    <script defer type="text/javascript" src="/js/index.js"></script>
    <script type="text/javascript">
    $(document).ready(function () {
      var cb = null;
      var els = $(".post figure.highlight");
      if (els.length) {
        // Enabled Hexo highlight line number.
        $(els).each(function (i, e) {
          $(e).before("<button class=\"copy button\">复制</button>");
        });
        cb = new ClipboardJS("button.copy", {
          "target": function (trigger) {
              // Get target element by DOM API.
              // nextElementSibling is figure,highlight.
              // And following is the sequence of Hexo's internal
              // highlight layout with line number.
              return trigger.nextElementSibling.firstChild.firstChild.firstChild.lastChild.firstChild.firstChild;
          }
        });
      } else {
        // Disabled Hexo highlight line number.
        els = $(".post pre code");
        $(els).each(function (i, e) {
          // Add button before pre, not code.
          $(e).parent().before("<button class=\"copy button\">复制</button>");
        });
        cb = new ClipboardJS("button.copy", {
          "target": function (trigger) {
              // Get target element by DOM API.
              // nextElementSibling is figure,highlight.
              // And following is the sequence of Hexo's internal
              // highlight layout without line number.
              return trigger.nextElementSibling.firstChild;
          }
        });
      }
      cb.on("success", function (e) {
        e.clearSelection();
        var trigger = e.trigger;
        // Change button text as a user tip.
        trigger.innerHTML = "已复制";
        $(trigger).addClass("copied");
        // Change button text back;
        setTimeout(function () {
          trigger.innerHTML = "复制";
          $(trigger).removeClass("copied");
        }, 1500);
      });
    });
    </script>
    
    <script defer type="text/javascript" src="/js/custom.js"></script>
    <title>How a Kalman filter works, in pictures | Huris' HeapStack - For you, a thousand times over.</title>
  </head>
  <body itemscope itemtype="http://schema.org/WebPage" lang="zh_CN" data-spy="scroll" data-target=".list-group">
    
<header id="header" class="header" style="background: #33363b;">
  <div class="container">
    <div class="header-container">
      <div class="header-title">
        <h1 class="title"><a href="/">Huris' HeapStack</a></h1>
        <h2 class="subtitle">For you, a thousand times over.</h2>
      </div>
      
      <div class="logo">
        <img src="/images/logo.png" alt="logo">
      </div>
      
    </div>
    <nav id="nav" class="nav">
      <a id="nav-toggle" class="nav-toggle" aria-hidden="true"><i class="fas fa-bars" aria-label="切换导航栏"></i></a>
      <ul id="menu" role="menubar" aria-hidden="false">
        
        <li role="menuitem"><a href="/"><i class="fas fa-home"></i><span class="menu-text">首页</span></a></li>
        
        <li role="menuitem"><a href="/archives/"><i class="fas fa-archive"></i><span class="menu-text">归档</span></a></li>
        
        <li role="menuitem"><a href="/categories/"><i class="fas fa-th-list"></i><span class="menu-text">分类</span></a></li>
        
        <li role="menuitem"><a href="/tags/"><i class="fas fa-tags"></i><span class="menu-text">标签</span></a></li>
        
        <li role="menuitem"><a href="/about/"><i class="fas fa-user-edit"></i><span class="menu-text">关于</span></a></li>
        
      </ul>
    </nav>
  </div>
</header>


    <main id="main" class="main">
      <div class="container">
        <div class="main-container">
          <div class="content">
            
<div id="post" class="page">
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="http://huris.xyz/2019/08/30/How-a-Kalman-filter-works-in-pictures/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
       <meta itemprop="name" content="Huris">
       <meta itemprop="description" content="To be is to be perceived.">
       <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
       <meta itemprop="name" content="Huris' HeapStack">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">How a Kalman filter works, in pictures</h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2019-08-30T16:38:41+08:00">2019-08-30 16:38:41</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/常见模型/" itemprop="url" rel="index"><span itemprop="name">常见模型</span></a></span><i class="fas fa-angle-right"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/常见模型/卡尔曼滤波/" itemprop="url" rel="index"><span itemprop="name">卡尔曼滤波</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      <p>I have to tell you about the Kalman filter, because what it does is pretty damn amazing.</p>
<p>Surprisingly few software engineers and scientists seem to know about it, and that makes me sad because it is such a general and powerful tool for <strong>combining information</strong> in the presence of uncertainty. At times its ability to extract accurate information seems almost magical— and if it sounds like I’m talking this up too much, then take a look at <a href="https://www.bzarg.com/p/improving-imu-attitude-estimates-with-velocity-data" target="_blank" rel="noopener">this previously posted video</a> where I demonstrate a Kalman filter figuring out the <em>orientation</em> of a free-floating body by looking at its <em>velocity</em>. Totally neat!</p>
<a id="more"></a>
<h2 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h2><p>You can use a Kalman filter in any place where you have <strong>uncertain information</strong> about some dynamic system, and you can make an <strong>educated guess</strong> about what the system is going to do next. Even if messy reality comes along and interferes with the clean motion you guessed about, the Kalman filter will often do a very good job of figuring out what actually happened. And it can take advantage of correlations between crazy phenomena that you maybe wouldn’t have thought to exploit!</p>
<p>Kalman filters are ideal for systems which are <strong>continuously changing</strong>. They have the advantage that they are light on memory (they don’t need to keep any history other than the previous state), and they are very fast, making them well suited for real time problems and embedded systems.</p>
<p>The math for implementing the Kalman filter appears pretty scary and opaque in most places you find on Google. That’s a bad state of affairs, because the Kalman filter is actually super simple and easy to understand if you look at it in the right way. Thus it makes a great article topic, and I will attempt to illuminate it with lots of clear, pretty pictures and colors. The prerequisites are simple; all you need is a basic understanding of probability and matrices.</p>
<p>I’ll start with a loose example of the kind of thing a Kalman filter can solve, but if you want to get right to the shiny pictures and math, feel free to <a href="http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/#mathybits" target="_blank" rel="noopener">jump ahead</a>.</p>
<h2 id="What-can-we-do-with-a-Kalman-filter"><a href="#What-can-we-do-with-a-Kalman-filter" class="headerlink" title="What can we do with a Kalman filter?"></a>What can we do with a Kalman filter?</h2><p>Let’s make a toy example: You’ve built a little robot that can wander around in the woods, and the robot needs to know exactly where it is so that it can navigate.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/robot_forest.png" width="30%"></p>
<p>We’ll say our robot has a state $\vec{x_k}$ , which is just a position and a velocity :</p>
<script type="math/tex; mode=display">
\nonumber\vec{x_k} = (\vec{p}, \vec{v})</script><p>Note that the state is just a list of numbers about the underlying configuration of your system; it could be anything. In our example it’s position and velocity, but it could be data about the amount of fluid in a tank, the temperature of a car engine, the position of a user’s finger on a touchpad, or any number of things you need to keep track of.</p>
<p>Our robot also has a GPS sensor, which is accurate to about 10 meters, which is good, but it needs to know its location more precisely than 10 meters. There are lots of gullies and cliffs in these woods, and if the robot is wrong by more than a few feet, it could fall off a cliff. So GPS by itself is not good enough.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/robot_ohnoes.png" width="30%"></p>
<p>We might also know something about how the robot moves: It knows the commands sent to the wheel motors, and its knows that if it’s headed in one direction and nothing interferes, at the next instant it will likely be further along that same direction. But of course it doesn’t know everything about its motion: It might be buffeted by the wind, the wheels might slip a little bit, or roll over bumpy terrain; so the amount the wheels have turned might not exactly represent how far the robot has actually traveled, and the prediction won’t be perfect.</p>
<p>The GPS <strong>sensor</strong> tells us something about the state, but only indirectly, and with some uncertainty or inaccuracy. Our <strong>prediction</strong> tells us something about how the robot is moving, but only indirectly, and with some uncertainty or inaccuracy.</p>
<p>But if we use all the information available to us, can we get a better answer than <strong>either estimate would give us by itself</strong>? Of course the answer is yes, and that’s what a Kalman filter is for.</p>
<h2 id="How-a-Kalman-filter-sees-your-problem"><a href="#How-a-Kalman-filter-sees-your-problem" class="headerlink" title="How a Kalman filter sees your problem"></a>How a Kalman filter sees your problem</h2><p>Let’s look at the landscape we’re trying to interpret. We’ll continue with a simple state having only position and velocity.</p>
<script type="math/tex; mode=display">
\nonumber\vec{x} = \begin{bmatrix}
p\\
v
\end{bmatrix}</script><p>We don’t know what the <em>actual</em> position and velocity are; there are a whole range of possible combinations of position and velocity that might be true, but some of them are more likely than others:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_0.png" width="30%"></p>
<p>The Kalman filter assumes that both variables (postion and velocity, in our case) are random and <em>Gaussian distributed.</em> Each variable has a <strong>mean</strong> value $\mu$), which is the center of the random distribution (and its most likely state), and a <strong>variance </strong>$\sigma^2$, which is the uncertainty:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_1.png" width="30%"></p>
<p>In the above picture, position and velocity are <strong>uncorrelated</strong>, which means that the state of one variable tells you nothing about what the other might be.</p>
<p>The example below shows something more interesting: Position and velocity are <strong>correlated</strong>. The likelihood of observing a particular position depends on what velocity you have:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_3.png" width="30%"></p>
<p>This kind of situation might arise if, for example, we are estimating a new position based on an old one. If our velocity was high, we probably moved farther, so our position will be more distant. If we’re moving slowly, we didn’t get as far.</p>
<p>This kind of relationship is really important to keep track of, because it gives us <strong>more information:</strong> One measurement tells us something about what the others could be. And that’s the goal of the Kalman filter, we want to squeeze as much information from our uncertain measurements as we possibly can!</p>
<p>This correlation is captured by something called a <a href="https://en.wikipedia.org/wiki/Covariance_matrix" target="_blank" rel="noopener">covariance matrix</a>. In short, each element of the matrix $\Sigma_{ij}$ is the degree of correlation between the <em>ith</em> state variable and the <em>jth</em> state variable. (You might be able to guess that the covariance matrix is <a href="https://en.wikipedia.org/wiki/Symmetric_matrix" target="_blank" rel="noopener">symmetric</a>, which means that it doesn’t matter if you swap <em>i</em> and <em>j</em>). Covariance matrices are oftimg en labelled “$\mathbf{\Sigma}$”, so we call their elements “$\Sigma_{ij}$”.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_2.png" width="30%"></p>
<h2 id="Describing-the-problem-with-matrices"><a href="#Describing-the-problem-with-matrices" class="headerlink" title="Describing the problem with matrices"></a>Describing the problem with matrices</h2><p>We’re modeling our knowledge about the state as a Gaussian blob, so we need two pieces of information at time (k): We’ll call our best estimate $\mathbf{\hat{x}_k}$ (the mean, elsewhere named $\mu$, and its covariance matrix $\mathbf{P_k}$. </p>
<script type="math/tex; mode=display">
\begin{equation} \label{eq:statevars}
\begin{aligned}
\mathbf{\hat{x}}_k &= \begin{bmatrix}
\text{position}\\
\text{velocity}
\end{bmatrix}\\
\mathbf{P}_k &=
\begin{bmatrix}
\Sigma_{pp} & \Sigma_{pv} \\
\Sigma_{vp} & \Sigma_{vv} \\
\end{bmatrix}
\end{aligned}
\end{equation}</script><p>(Of course we are using only position and velocity here, but it’s useful to remember that the state can contain any number of variables, and represent anything you want).</p>
<p>Next, we need some way to look at the <strong>current state</strong> (at time <strong>k-1</strong>) and <strong>predict the next state</strong> at time <strong>k</strong>. Remember, we don’t know which state is the “real” one, but our prediction function doesn’t care. It just works on <em>all of them</em>, and gives us a new distribution:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_7.jpg" width="30%"></p>
<p>We can represent this prediction step with a matrix, $\mathbf{F_k}$:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_8.jpg" width="30%"></p>
<p>It takes <em>every point</em> in our original estimate and moves it to a new predicted location, which is where the system would move if that original estimate was the right one.</p>
<p>Let’s apply this. How would we use a matrix to predict the position and velocity at the next moment in the future? We’ll use a really basic kinematic formula:</p>
<script type="math/tex; mode=display">
\nonumber\begin{split}
\color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + \Delta t &\color{royalblue}{v_{k-1}} \\
\color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}}
\end{split}</script><p>In other words : </p>
<script type="math/tex; mode=display">
\begin{align}
\color{deeppink}{\mathbf{\hat{x}}_k} &= \begin{bmatrix}
1 & \Delta t \\
0 & 1
\end{bmatrix} \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\
&= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \label{statevars}
\end{align}</script><p>We now have a <strong>prediction matrix</strong> which gives us our next state, but we still don’t know how to update the covariance matrix.</p>
<p>This is where we need another formula. If we multiply every point in a distribution by a matrix $\color{firebrick}{\mathbf{A}}$, then what happens to its covariance matrix $\Sigma$ ?</p>
<p>Well, it’s easy. I’ll just give you the identity:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
Cov(x) &= \Sigma\\
Cov(\color{firebrick}{\mathbf{A}}x) &= \color{firebrick}{\mathbf{A}} \Sigma \color{firebrick}{\mathbf{A}}^T
\end{split} \label{covident}
\end{equation}</script><p>So combining $\eqref{covident}$ with equation $\eqref{statevars}$:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\
\color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T
\end{split}
\end{equation}</script><h2 id="External-influence"><a href="#External-influence" class="headerlink" title="External influence"></a>External influence</h2><p>We haven’t captured everything, though. There might be some changes that <strong>aren’t related to the state</strong> itself— the outside world could be affecting the system.</p>
<p>For example, if the state models the motion of a train, the train operator might push on the throttle, causing the train to accelerate. Similarly, in our robot example, the navigation software might issue a command to turn the wheels or stop. If we know this additional information about what’s going on in the world, we could stuff it into a vector called $\color{darkorange}{\vec{\mathbf{u}_k}}$, do something with it, and add it to our prediction as a correction.</p>
<p>Let’s say we know the expected acceleration $\color{darkorange}{a}$ due to the throttle setting or control commands. From basic kinematics we get :</p>
<script type="math/tex; mode=display">
\nonumber\begin{split}
\color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + {\Delta t} &\color{royalblue}{v_{k-1}} + &\frac{1}{2} \color{darkorange}{a} {\Delta t}^2 \\
\color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} + & \color{darkorange}{a} {\Delta t}
\end{split}</script><p>In matrix form: </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \begin{bmatrix}
\frac{\Delta t^2}{2} \\
\Delta t
\end{bmatrix} \color{darkorange}{a} \\
&= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}}
\end{split}
\end{equation}</script><p>$\mathbf{B}_k$ is called the <strong>control matrix</strong> and $\color{darkorange}{\vec{\mathbf{u}_k}}$ the <strong>control vector.</strong></p>
<p>(For very simple systems with no external influence, you could omit these).</p>
<p>Let’s add one more detail. What happens if our prediction is not a 100% accurate model of what’s actually going on?</p>
<h2 id="External-uncertainty"><a href="#External-uncertainty" class="headerlink" title="External uncertainty"></a>External uncertainty</h2><p>Everything is fine if the state evolves based on its own properties. Everything is <em>still</em> fine if the state evolves based on external forces, so long as we know what those external forces are.</p>
<p>But what about forces that we <em>don’t</em> know about? If we’re tracking a quadcopter, for example, it could be buffeted around by wind. If we’re tracking a wheeled robot, the wheels could slip, or bumps on the ground could slow it down. We can’t keep track of these things, and if any of this happens, our prediction could be off because we didn’t account for those extra forces.</p>
<p>We can model the uncertainty associated with the “world” (i.e. things we aren’t keeping track of) by adding some new uncertainty after every prediction step:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_9.jpg" width="30%"></p>
<p>Every state in our original estimate could have moved to a <em>range</em> of states. Because we like Gaussian blobs so much, we’ll say that each point in $\color{royalblue}{\mathbf{\hat{x}}_{k-1}}$ is moved to somewhere inside a Gaussian blob with covariance $\color{mediumaquamarine}{\mathbf{Q}_k}$. Another way to say this is that we are treating the untracked influences as <strong>noise</strong> with covariance $\color{mediumaquamarine}{\mathbf{Q}_k}$.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_10a.jpg" width="30%"></p>
<p>This produces a new Gaussian blob, with a different covariance (but the same mean):</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_10b.jpg" width="30%"></p>
<p>We get the expanded covariance by simply <strong>adding</strong> ${\color{mediumaquamarine}{\mathbf{Q}_k}}$, giving our complete expression for the <strong>prediction step</strong>: </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \\
\color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T + \color{mediumaquamarine}{\mathbf{Q}_k}
\end{split}
\label{kalpredictfull}
\end{equation}</script><p>In other words, the <strong>new best estimate</strong> is a <strong>prediction</strong> made from <strong>previous best estimate</strong>, plus a <strong>correction</strong> for <strong>known external influences</strong>.</p>
<p>And the <strong>new uncertainty</strong> is <strong>predicted</strong> from the <strong>old uncertainty</strong>, with some <strong>additional uncertainty from the environment</strong>.</p>
<p>All right, so that’s easy enough. We have a fuzzy estimate of where our system might be, given by $\color{deeppink}{\mathbf{\hat{x}}_k}$ and $\color{deeppink}{\mathbf{P}_k}$. What happens when we get some data from our sensors?</p>
<h2 id="Refining-the-estimate-with-measurements"><a href="#Refining-the-estimate-with-measurements" class="headerlink" title="Refining the estimate with measurements"></a>Refining the estimate with measurements</h2><p>We might have several sensors which give us information about the state of our system. For the time being it doesn’t matter what they measure; perhaps one reads position and the other reads velocity. Each sensor tells us something <strong>indirect</strong> about the state— in other words, the sensors operate on a state and produce a set of <strong>readings</strong>.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_12.jpg" width="30%"></p>
<p>Notice that the units and scale of the reading might not be the same as the units and scale of the state we’re keeping track of. You might be able to guess where this is going: We’ll model the sensors with a matrix, $\mathbf{H}_k$.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_13.jpg" width="30%"></p>
<p>We can figure out the distribution of sensor readings we’d expect to see in the usual way:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\vec{\mu}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{\hat{x}}_k} \\
\mathbf{\Sigma}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{P}_k} \mathbf{H}_k^T
\end{aligned}
\end{equation}</script><p>One thing that Kalman filters are great for is dealing with <em>sensor noise</em>. In other words, our sensors are at least somewhat unreliable, and every state in our original estimate might result in a <em>range</em> of sensor readings.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_14.jpg" width="30%"></p>
<p>From each reading we observe, we might guess that our system was in a particular state. But because there is uncertainty, <strong>some states are more likely than others</strong> to have have produced the reading we saw:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_11.jpg" width="30%"></p>
<p>We’ll call the <strong>covariance</strong> of this uncertainty (i.e. of the sensor noise) $\color{mediumaquamarine}{\mathbf{R}_k}$. The distribution has a <strong>mean</strong> equal to the reading we observed, which we’ll call $\color{yellowgreen}{\vec{\mathbf{z}_k}}$.</p>
<p>So now we have two Gaussian blobs: One surrounding the mean of our transformed prediction, and one surrounding the actual sensor reading we got.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_4.jpg" width="30%"></p>
<p>We must try to reconcile our guess about the readings we’d see based on the <strong>predicted state</strong> (<strong>pink</strong>) with a <em>different</em> guess based on our <strong>sensor readings</strong> (<strong>green</strong>) that we actually observed.</p>
<p>So what’s our new most likely state? For any possible reading $(z_1,z_2)$, we have two associated probabilities: (1) The probability that our sensor reading $\color{yellowgreen}{\vec{\mathbf{z}_k}}$ is a (mis-)measurement of $(z_1,z_2)$, and (2) the probability that our previous estimate thinks $(z_1,z_2)$ is the reading we should see.</p>
<p>If we have two probabilities and we want to know the chance that <em>both</em> are true, we just multiply them together. So, we take the two Gaussian blobs and multiply them:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_6a.png" width="30%"></p>
<p>What we’re left with is the <strong>overlap</strong>, the region where <em>both</em> blobs are bright/likely. And it’s a lot more precise than either of our previous estimates. The mean of this distribution is the configuration for which <strong>both estimates are most likely</strong>, and is therefore the <strong>best guess</strong>of the true configuration given all the information we have.</p>
<p>Hmm. This looks like another Gaussian blob.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_6.png" width="30%"></p>
<p>As it turns out, when you multiply two Gaussian blobs with separate means and covariance matrices, you get a <em>new</em> Gaussian blob with its <strong>own</strong> mean and covariance matrix! Maybe you can see where this is going: There’s got to be a formula to get those new parameters from the old ones!</p>
<h2 id="Combining-Gaussians"><a href="#Combining-Gaussians" class="headerlink" title="Combining Gaussians"></a>Combining Gaussians</h2><p>Let’s find that formula. It’s easiest to look at this first in <strong>one dimension</strong>. A 1D Gaussian bell curve with variance $\sigma^2$ and mean (\mu) is defined as: </p>
<script type="math/tex; mode=display">
\begin{equation} \label{gaussformula}
\mathcal{N}(x, \mu,\sigma) = \frac{1}{ \sigma \sqrt{ 2\pi } } e^{ -\frac{ (x – \mu)^2 }{ 2\sigma^2 } }
\end{equation}</script><p>We want to know what happens when you multiply two Gaussian curves together. The blue curve below represents the (unnormalized) intersection of the two Gaussian populations:</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/gauss_joint.png" width="30%"></p>
<script type="math/tex; mode=display">
\begin{equation} \label{gaussequiv}
\mathcal{N}(x, \color{fuchsia}{\mu_0}, \color{deeppink}{\sigma_0}) \cdot \mathcal{N}(x, \color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\sigma_1}) \stackrel{?}{=} \mathcal{N}(x, \color{royalblue}{\mu’}, \color{mediumblue}{\sigma’})
\end{equation}</script><p>You can substitute equation $\eqref{gaussformula}$ into equation $\eqref{gaussequiv}$ and do some algebra (being careful to renormalize, so that the total probability is 1) to obtain: </p>
<script type="math/tex; mode=display">
\begin{equation} \label{fusionformula}
\begin{aligned}
\color{royalblue}{\mu’} &= \mu_0 + \frac{\sigma_0^2 (\mu_1 – \mu_0)} {\sigma_0^2 + \sigma_1^2}\\
\color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – \frac{\sigma_0^4} {\sigma_0^2 + \sigma_1^2}
\end{aligned}
\end{equation}</script><p>We can simplify by factoring out a little piece and calling it $\color{purple}{\mathbf{k}}$: </p>
<script type="math/tex; mode=display">
\begin{equation} \label{gainformula}
\color{purple}{\mathbf{k}} = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_1^2}
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{royalblue}{\mu’} &= \mu_0 + &\color{purple}{\mathbf{k}} (\mu_1 – \mu_0)\\
\color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – &\color{purple}{\mathbf{k}} \sigma_0^2
\end{split} \label{update}
\end{equation}</script><p>Take note of how you can take your previous estimate and <strong>add something</strong> to make a new estimate. And look at how simple that formula is!</p>
<p>But what about a matrix version? Well, let’s just re-write equations $\eqref{gainformula}$ and $\eqref{update}$ in matrix form. If $\Sigma$ is the covariance matrix of a Gaussian blob, and $\vec{\mu}$ its mean along each axis, then: </p>
<script type="math/tex; mode=display">
\begin{equation} \label{matrixgain}
\color{purple}{\mathbf{K}} = \Sigma_0 (\Sigma_0 + \Sigma_1)^{-1}
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{royalblue}{\vec{\mu}’} &= \vec{\mu_0} + &\color{purple}{\mathbf{K}} (\vec{\mu_1} – \vec{\mu_0})\\
\color{mediumblue}{\Sigma’} &= \Sigma_0 – &\color{purple}{\mathbf{K}} \Sigma_0
\end{split} \label{matrixupdate}
\end{equation}</script><p>$\color{purple}{\mathbf{K}}$ is a matrix called the <strong>Kalman gain</strong>, and we’ll use it in just a moment.</p>
<p>Easy! We’re almost finished!</p>
<h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h2><p>We have two distributions: The predicted measurement with $(\color{fuchsia}{\mu_0}, \color{deeppink}{\Sigma_0}) = (\color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k}, \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T}) $, and the observed measurement with $(\color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\Sigma_1}) = (\color{yellowgreen}{\vec{\mathbf{z}_k}}, \color{mediumaquamarine}{\mathbf{R}_k})$. We can just plug these into equation $\eqref{matrixupdate}$ to find their overlap:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathbf{H}_k \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\
\mathbf{H}_k \color{royalblue}{\mathbf{P}_k’} \mathbf{H}_k^T &= \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} & – & \color{purple}{\mathbf{K}} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T}
\end{aligned} \label {kalunsimplified}
\end{equation}</script><p> And from $\eqref{matrixgain}$, the Kalman gain is:</p>
<script type="math/tex; mode=display">
\begin{equation} \label{eq:kalgainunsimplified}
\color{purple}{\mathbf{K}} = \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1}
\end{equation}</script><p> We can knock an $\mathbf{H}_k$ off the front of every term in $\eqref{kalunsimplified}$ and $\eqref{eq:kalgainunsimplified}$ (note that one is hiding inside $\color{purple}{\mathbf{K}}$ ), and an $\mathbf{H}_k^T$ off the end of all terms in the equation for $\color{royalblue}{\mathbf{P}_k’}$. </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}’} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\
\color{royalblue}{\mathbf{P}_k’} &= \color{deeppink}{\mathbf{P}_k} & – & \color{purple}{\mathbf{K}’} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k}
\end{split}
\label{kalupdatefull}
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\color{purple}{\mathbf{K}’} = \color{deeppink}{\mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1}
\label{kalgainfull}
\end{equation}</script><p> …giving us the complete equations for the <strong>update step.</strong></p>
<p>And that’s it! $\color{royalblue}{\mathbf{\hat{x}}_k’}$ is our new best estimate, and we can go on and feed it (along with  $\color{royalblue}{\mathbf{P}_k’} $ ) back into another round of <strong>predict</strong> or <strong>update</strong> as many times as we like.</p>
<p><img src="https://huris.oss-cn-hangzhou.aliyuncs.com/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/kalflow.png" width="80%"></p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Of all the math above, all you need to implement are equations $\eqref{kalpredictfull}, \eqref{kalupdatefull}$, and $\eqref{kalgainfull}$. (Or if you forget those, you could re-derive everything from equations $\eqref{covident}$ and $\eqref{matrixupdate}$.)</p>
<p>This will allow you to model any linear system accurately. For nonlinear systems, we use the <strong>extended Kalman filter</strong>, which works by simply linearizing the predictions and measurements about their mean. (I may do a second write-up on the EKF in the future).</p>
<p>If I’ve done my job well, hopefully someone else out there will realize how cool these things are and come up with an unexpected new place to put them into action.</p>
<p>Some credit and referral should be given to <a href="http://www.cl.cam.ac.uk/~rmf25/papers/Understanding the Basis of the Kalman Filter.pdf" target="_blank" rel="noopener">this fine document</a>, which uses a similar approach involving overlapping Gaussians. More in-depth derivations can be found there, for the curious.</p>

    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/卡尔曼滤波/" rel="tag"><i class="fas fa-tags"></i>卡尔曼滤波</a>
        
      </div>
      
    </footer>
  </article>
  
  
<div class="reward" id="reward">
  <p>把最实用的经验，分享给最需要的读者，希望每一位来访的朋友都能有所收获！</p>
  <button id="reward-button" class="button" disable="enable">打赏</button>
  <div id="qr" class="qr" style="display: none;" aria-hidden="true">
    
    <div id="wechat">
      <img id="wechat_qr" src="/images/WeChatPay.jpg" alt="微信支付">
      <span>微信支付</span>
    </div>
    
    
    <div id="alipay">
      <img id="alipay_qr" src="/images/Alipay.jpg" alt="支付宝">
      <span>支付宝</span>
    </div>
    
    
  </div>
</div>


  
  
  <nav class="page-nav">
    <div class="page-nav-next page-nav-item">
      
      <a href="/2019/08/28/leetcode-101-对称二叉树/" rel="next" title="leetcode-101-对称二叉树"><i class="fas fa-angle-left"></i><span class="nav-title">leetcode-101-对称二叉树</span></a>
      
    </div>
    <div class="page-nav-prev page-nav-item">
      
      <a href="/2019/09/01/卷积神经网络（CNN）模型结构可视化工具/" rel="prev" title="卷积神经网络（CNN）模型结构可视化工具"><span class="nav-title">卷积神经网络（CNN）模型结构可视化工具</span><i class="fas fa-angle-right"></i></a>
      
    </div>
  </nav>
  
  
</div>

          </div>
          
          
          
<aside class="sidebar" id="sidebar" style="background: url(/images/background.png);">
  
  <div class="search">
    <div class="form-group">
      <i class="fas fa-search"></i><input type="search" id="search-input" name="q" results="0" placeholder="搜索" class="form-control">
    </div>
  </div>
  <div class="search-result-box" id="search-result"></div>
  
  
<div class="info sidebar-item" id="info">
  
  <img class="author-avatar" src="/images/avatar.jpg" alt="Huris">
  
  <h1 class="author-name">Huris</h1>
  <h2 class="author-description">To be is to be perceived.</h2>
  <div class="site-count">
    
    
    
    
    <div class="archives-count count-block">
      <div class="site-count-title">归档</div>
      <div><a href="/archives/">86</a></div>
    </div>
    
    
    
    <div class="categories-count count-block">
      <div class="site-count-title">分类</div>
      <div><a href="/categories/">48</a></div>
    </div>
    
    
    
    <div class="tags-count count-block">
      <div class="site-count-title">标签</div>
      <div><a href="/tags/">89</a></div>
    </div>
    
    
    
    
  </div>
  
  <div class="rss">
    <a class="rss-link button sidebar-item" href="/atom.xml"><i class="fas fa-rss"></i>RSS</a>
  </div>
  
</div>


  <div class="sidebar-sticky">
    
    
    
    
    
    <hr>
    <div class="post-toc sidebar-item" id="toc-div">
      <div><i class="fas fa-list-ol"></i>文章目录</div>
      <div class="post-toc-content"><ol class="list-group toc"><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#What-is-it"><span class="toc-text">What is it?</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#What-can-we-do-with-a-Kalman-filter"><span class="toc-text">What can we do with a Kalman filter?</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#How-a-Kalman-filter-sees-your-problem"><span class="toc-text">How a Kalman filter sees your problem</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#Describing-the-problem-with-matrices"><span class="toc-text">Describing the problem with matrices</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#External-influence"><span class="toc-text">External influence</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#External-uncertainty"><span class="toc-text">External uncertainty</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#Refining-the-estimate-with-measurements"><span class="toc-text">Refining the estimate with measurements</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#Combining-Gaussians"><span class="toc-text">Combining Gaussians</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#Putting-it-all-together"><span class="toc-text">Putting it all together</span></a></li><li class="toc-item toc-level-2"><a class="list-group-item toc-link" href="#Wrapping-up"><span class="toc-text">Wrapping up</span></a></li></ol></div>
    </div>
    
    
    
    <hr>
    <div class="social-link sidebar-item">
      <div><i class="far fa-address-card"></i>社交链接<p></p></div>
      <ul>
        
        <li><i class="fas fa-envelope"></i><a href="mailto:huris102234@gmail.com" target="_blank">E-Mail</a></li>
        
        <li><i class="fab fa-github"></i><a href="https://github.com/huris" target="_blank">GitHub</a></li>
        
        <li><i class="fab fa-zhihu"></i><a href="https://www.zhihu.com/people/xia-zhen-15-24/activities" target="_blank">Zhihu</a></li>
        
      </ul>
    </div>
    
    
    <hr>
    <div class="blogroll sidebar-item">
      <div><i class="fas fa-link"></i>友情链接</div>
      <ul>
        
        <li><i class="fab fa-weebly"></i><a href="https://fontawesome.com/" target="_blank">Font Awesome</a></li>
        
        <li><i class="fab fa-weebly"></i><a href="http://www.yishimei.cn/" target="_blank">亦是美网络</a></li>
        
        <li><i class="fab fa-weebly"></i><a href="http://bigjpg.com/" target="_blank">Bigjpg</a></li>
        
        <li><i class="fab fa-weebly"></i><a href="https://pixabay.com/" target="_blank">pixabay</a></li>
        
        <li><i class="fab fa-weebly"></i><a href="https://tinypng.com/" target="_blank">tinypng</a></li>
        
      </ul>
    </div>
    
  </div>
</aside>


          
        </div>
      </div>
    </main>
    
<footer id="footer" class="footer" style="background: #33363b;">
  <div class="container">
    <div class="back-to-top">
      <button id="back-to-top"><i class="fas fa-angle-double-up" aria-label="回到顶部"></i></button>
    </div>
    <div class="footer-container">
      <div class="footer-left">
        <div class="copyright">
          <span class="author">Huris</span><span class="year"><i class="far fa-copyright"></i>2017 - 2020</span><span class="creative-commons"><i class="fab fa-creative-commons"></i><a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">BY-NC 4.0</a></span>
        </div>
        
        <div class="busuanzi">
          <span id="busuanzi_container_site_pv"><i class="fas fa-eye" aria-label="站点点击量" aria-hidden="false"></i><span id="busuanzi_value_site_pv"></span></span><span id="busuanzi_container_site_uv"><i class="fas fa-user" aria-label="站点用户数" aria-hidden="false"></i><span id="busuanzi_value_site_uv"></span></span><span id="busuanzi_container_page_pv"><i class="far fa-file-alt"></i><span id="busuanzi_value_page_pv" aria-label="页面点击量" aria-hidden="false"></span></span>
        </div>
        
      </div>
      <div class="footer-right">
        <div class="custom-info">
          
          托管于<i class="fab fa-github-alt"></i><a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
          
        </div>
        <div class="powered-by">
          由 <a href="https://hexo.io/" target="_blank">Hexo</a> 强力驱动 | 主题 <a href="https://github.com/AlynxZhou/hexo-theme-aria/" target="_blank">ARIA</a>
        </div>
      </div>
    </div>
  </div>
</footer>


  </body>
</html>
